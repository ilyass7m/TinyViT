# TinyViT-21M Teacher for CIFAR-100 Distillation
# Step 1: Fine-tune this as teacher, then save logits
# Usage: torchrun --nproc_per_node=2 main.py --cfg configs/cifar100/tiny_vit_21m_cifar100_teacher.yaml --pretrained pretrained/tiny_vit_21m_22k_distill.pth

MODEL:
  NAME: TinyViT-21M-CIFAR100-Teacher
  TYPE: tiny_vit
  NUM_CLASSES: 100
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1

  TINY_VIT:
    DEPTHS: [ 2, 2, 6, 2 ]
    NUM_HEADS: [ 3, 6, 12, 24 ]
    WINDOW_SIZES: [ 7, 7, 14, 7 ]
    EMBED_DIMS: [ 96, 192, 384, 576 ]

DATA:
  DATASET: cifar100
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  BATCH_SIZE: 256
  NUM_WORKERS: 8
  PIN_MEMORY: True

TRAIN:
  # Fine-tuning for teacher
  EPOCHS: 30
  WARMUP_EPOCHS: 3
  BASE_LR: 2e-4
  WARMUP_LR: 1e-7
  MIN_LR: 1e-6
  WEIGHT_DECAY: 1e-8
  CLIP_GRAD: 5.0
  LAYER_LR_DECAY: 0.75
  AUTO_RESUME: True
  EVAL_BN_WHEN_TRAINING: True

AUG:
  # Light augmentation for teacher fine-tuning
  COLOR_JITTER: 0.3
  AUTO_AUGMENT: rand-m5-mstd0.5-inc1
  REPROB: 0.1
  REMODE: pixel
  RECOUNT: 1
  MIXUP: 0.0
  CUTMIX: 0.0

TEST:
  CROP: True

PRINT_FREQ: 20
SAVE_FREQ: 5
